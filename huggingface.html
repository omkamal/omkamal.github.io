<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hugging Face Hub Client Library Cheatsheet</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400&display=swap" rel="stylesheet"> 
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            font-size: 10px;
            line-height: 1.2;
            margin: 0;
            padding: 10mm;
            background-color: #f0f8ff;
            color: #333;
            box-sizing: border-box;
        }
        
        .container {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            grid-gap: 10px;
            width: 100%;
        }

        .container2 {
            display: grid;
            grid-template-columns: repeat(2, 1fr);
            grid-gap: 10px;
            width: 100%;
        }
        
        .container4 {
            display: grid;
            grid-template-columns: repeat(4, 1fr);
            grid-gap: 10px;
            width: 100%;
        }
        
        h1 {
            grid-column: 1 / -1;
            text-align: center;
            font-size: 24px;
            margin: 0 0 10px 0;
            color: #2c3e50;
            text-shadow: 1px 1px 2px rgba(0,0,0,0.1);
        }
        
        h2 {
            font-size: 14px;
            margin: 0 0 5px 0;
            color: #e74c3c;
            border-bottom: 2px solid #e74c3c;
            padding-bottom: 3px;
        }
        
        ul {
            margin: 0;
            padding-left: 15px;
        }
        
        li {
            margin-bottom: 3px;
        }
        
        .emoji {
            font-style: normal;
        }
        
        .author {
            position: absolute;
            top: 5px;
            left: 5px;
            font-size: 8px;
            color: #7f8c8d;
        }
        
        .linkedin {
            position: absolute;
            top: 5px;
            right: 5px;
            font-size: 8px;
        }
        
        .linkedin a {
            color: #0077b5;
            text-decoration: none;
        }
        
        .section, .svg-container, .img-container {
            background-color: #fff;
            border-radius: 8px;
            padding: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            overflow: hidden;
            box-sizing: border-box;
        }
        
        .section {
            display: flex;
            flex-direction: column;
            justify-content: flex-start;
        }
        
        .svg-container, .img-container {
            display: flex;
            justify-content: center;
            align-items: center;
            height: 150px; /* Set a fixed height to match text sections */
            position: relative;
        }
        
        strong {
            color: #2980b9;
        }
        
        .svg-container::before, .img-container::before {
            content: '';
            position: absolute;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(45deg, #f3f4f6 25%, transparent 25%, transparent 75%, #f3f4f6 75%, #f3f4f6),
                        linear-gradient(45deg, #f3f4f6 25%, transparent 25%, transparent 75%, #f3f4f6 75%, #f3f4f6);
            background-size: 10px 10px;
            background-position: 0 0, 5px 5px;
            opacity: 0.1;
            z-index: 1;
        }
        
        .svg-container svg, .img-container img {
            max-width: 100%;
            max-height: 100%;
            width: auto;
            height: auto;
            object-fit: contain;
            z-index: 2;
        }
        
        @media print {
            body {
                width: 297mm;
                padding: 10mm;
            }
        }
    </style>
</head>
<body>
    <div class="author">Omar Hosney</div>
    <div class="linkedin"><a href="https://www.linkedin.com/in/omarebnelkhattab-hosney-9a931b3/" target="_blank"><i class="fab fa-linkedin"></i> Linkedin Profile</a></div>
    <h1>ü§ó Hugging Face Hub Client Library Cheatsheet</h1>

    <div class="container">
        <div class="section">
            <h2>Quickstart üèÅ</h2>
            <p>The Hugging Face Hub is the <strong>central place</strong> to discover and share ML models, datasets, and Spaces.</p>
            <ul>
                <li><strong>Discover:</strong> Find pre-trained models and datasets üïµÔ∏è‚Äç‚ôÄÔ∏è.</li>
                <li><strong>Collaborate:</strong> Share and work together on ML projects ü§ù.</li>
                <li><strong>Explore:</strong> Try out ML apps and demos ‚ú®.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Installation üíª</h2>
            <p>Start by <strong>installing</strong> the huggingface_hub library:</p>
            <pre><code>pip install --upgrade huggingface_hub</code></pre>
            <p>For optional features, install extra dependencies like:</p>
            <pre><code>pip install 'huggingface_hub[tensorflow]'</code></pre>
        </div>
        <div class="section">
            <h2>Download Files üì•</h2>
            <p>Use <strong>hf_hub_download()</strong> to download a <strong>specific file</strong> from a repo:</p>
            <pre><code>from huggingface_hub import hf_hub_download
hf_hub_download(repo_id="google/pegasus-xsum", filename="config.json")
</code></pre>
            <p>For an <strong>entire repo</strong>, use <strong>snapshot_download()</strong>:</p>
            <pre><code>from huggingface_hub import snapshot_download
snapshot_download(repo_id="lysandre/arxiv-nlp")
</code></pre>
        </div>
    </div>
<br>
    <div class="container">
        <div class="section">
            <h2>Authentication üîê</h2>
            <p>You'll need a <strong>Hugging Face account</strong> and a <strong>User Access Token</strong> for many actions.</p>
            <ul>
                <li><strong>Login Command:</strong></li>
                <pre><code>huggingface-cli login</code></pre>
                <li><strong>Environment Variable:</strong> Set <code>HF_TOKEN</code>.</li>
                <li><strong>Method Parameters:</strong> Pass your token as an argument.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Create a Repo ‚ûï</h2>
            <p>Use <strong>create_repo()</strong> to create a new repository:</p>
            <pre><code>from huggingface_hub import HfApi
api = HfApi()
api.create_repo(repo_id="my-new-model")
</code></pre>
            <p>Set <code>private=True</code> for a private repo.</p>
        </div>
        <div class="section">
            <h2>Upload Files üì§</h2>
            <p>Use <strong>upload_file()</strong> for a <strong>single file</strong>:</p>
            <pre><code>api.upload_file(
    path_or_fileobj="/path/to/README.md",
    path_in_repo="README.md",
    repo_id="my-username/my-repo"
)
</code></pre>
            <p>For a <strong>folder</strong>, use <strong>upload_folder()</strong>.</p>
        </div>
    </div>
<br>
    <div class="container">
        <div class="section">
            <h2>Inference API üß†</h2>
            <p>Run <strong>accelerated inference</strong> on Hugging Face servers with <strong>InferenceClient</strong>:</p>
            <pre><code>from huggingface_hub import InferenceClient
client = InferenceClient()

# Text to Image
image = client.text_to_image("A cat wearing a hat")
image.save("cat_with_hat.png")

# Chat Completion
messages = [{"role": "user", "content": "Translate 'Hello' to Spanish."}]
response = client.chat_completion(messages, model="google/flan-t5-xl")
print(response.choices[0].message.content) # ¬°Hola!
</code></pre>
        </div>
        <div class="section">
            <h2>Manage Repos üìÅ</h2>
            <p>Use the <strong>HfApi</strong> to <strong>manage your repositories</strong>:</p>
            <ul>
                <li><strong>delete_repo()</strong>: Delete a repository.</li>
                <li><strong>update_repo_visibility()</strong>: Change visibility (public/private).</li>
                <li><strong>move_repo()</strong>: Rename or transfer a repo.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Discussions & Pull Requests üí¨</h2>
            <p>Interact with the <strong>community</strong> using <strong>HfApi</strong>:</p>
            <ul>
                <li><strong>get_repo_discussions()</strong>: Retrieve discussions and pull requests.</li>
                <li><strong>create_discussion()</strong>, <strong>create_pull_request()</strong>: Start new discussions or PRs.</li>
                <li><strong>comment_discussion()</strong>, <strong>merge_pull_request()</strong>: Participate in discussions.</li>
            </ul>
        </div>
    </div>
<br>
    <div class="container">
        <div class="section">
            <h2>Collections üìö</h2>
            <p><strong>Organize</strong> models, datasets, Spaces, and papers with <strong>collections</strong>.</p>
            <ul>
                <li><strong>get_collection()</strong>: Fetch a specific collection.</li>
                <li><strong>list_collections()</strong>: Browse available collections.</li>
                <li><strong>create_collection()</strong>, <strong>add_collection_item()</strong>: Build your own collections.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Cache Management üßπ</h2>
            <p>The <strong>huggingface_hub</strong> library <strong>caches downloaded files</strong> to speed up future access.</p>
            <ul>
                <li><strong>huggingface-cli scan-cache</strong>: Inspect your cache usage.</li>
                <li><strong>huggingface-cli delete-cache</strong>: Free up disk space by deleting cached items.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Model Cards üìù</h2>
            <p>Create informative <strong>Model Cards</strong> to describe your models.</p>
            <ul>
                <li><strong>ModelCard.load()</strong>, <strong>ModelCard.from_template()</strong>: Create or load Model Cards.</li>
                <li><strong>ModelCard.push_to_hub()</strong>: Share your Model Card on the Hub.</li>
                <li><strong>metadata_update()</strong>: Update your card‚Äôs metadata.</li>
            </ul>
        </div>
    </div>
    <br>
    
 <div class="container2">
    <div class="section">
        <h2>Login/Logout & Authentication üîê</h2>
        <ul>
            <li><strong>Log in</strong> using a user access token: <code>huggingface_hub.login(token, add_to_git_credential=False, new_session=True, write_permission=False)</code>.</li>
            <li>Log in using the <strong>terminal</strong>: <code>huggingface_hub.interpreter_login(new_session=True, write_permission=False)</code>.</li>
            <li>Log in using a <strong>notebook widget</strong>: <code>huggingface_hub.notebook_login(new_session=True, write_permission=False)</code>.</li>
            <li><strong>Log out</strong> and delete the saved token: <code>huggingface_hub.logout()</code>.</li>
        </ul>
    </div>

    <div class="section">
        <h2>Environment Variables üå≥</h2>
        <ul>
            <li><strong>Local data directory</strong> for Hugging Face Hub: <code>HF_HOME</code>.</li>
            <li><strong>Cache directory</strong> for models, datasets, and spaces: <code>HF_HUB_CACHE</code>.</li>
            <li><strong>Cache directory</strong> for downstream library assets: <code>HF_ASSETS_CACHE</code>.</li>
            <li>Your Hugging Face <strong>user access token</strong>: <code>HF_TOKEN</code>.</li>
            <li>Set to 1 to enable <strong>offline mode</strong>: <code>HF_HUB_OFFLINE</code>.</li>
            <li>Set to 1 to <strong>disable progress bars</strong>: <code>HF_HUB_DISABLE_PROGRESS_BARS</code>.</li>
            <li>Set to 1 to <strong>disable telemetry</strong>: <code>HF_HUB_DISABLE_TELEMETRY</code>.</li>
            <!-- Add more relevant environment variables -->
        </ul>
    </div>

   
</div>
<div class="container1">
    
 <div class="section">
        <h2><code>HfApi</code> Client üåê</h2>
        <ul>
            <li>Create an <strong>HfApi client</strong> to interact with the Hub: <code>huggingface_hub.HfApi(endpoint=None, token=None, ...)</code>.</li>
            <li><strong>Create</strong> a new repository: <code>api.create_repo(repo_id, token=None, private=False, repo_type=None, exist_ok=False, ...)</code>.</li>
            <li><strong>Delete</strong> a repository: <code>api.delete_repo(repo_id, token=None, repo_type=None, missing_ok=False)</code>.</li>
            <li><strong>Upload</strong> a file: <code>api.upload_file(path_or_fileobj, path_in_repo, repo_id, token=None, repo_type=None, revision=None, ...)</code>.</li>
            <li><strong>Upload</strong> a folder: <code>api.upload_folder(folder_path, path_in_repo=None, repo_id, token=None, repo_type=None, revision=None, ...)</code>.</li>
            <li><strong>Download</strong> a file: <code>api.hf_hub_download(repo_id, filename, revision=None, cache_dir=None, force_download=False, ...)</code>.</li>
            <li><strong>List</strong> available models: <code>api.list_models(filter=None, author=None, search=None, sort=None, direction=None, limit=None, ...)</code>.</li>
            <li><strong>List</strong> available datasets: <code>api.list_datasets(filter=None, author=None, search=None, sort=None, direction=None, limit=None, ...)</code>.</li>
            <li>Get <strong>model information</strong>: <code>api.model_info(repo_id, revision=None, timeout=None, securityStatus=None, files_metadata=False, ...)</code>.</li>
            <li>Get <strong>dataset information</strong>: <code>api.dataset_info(repo_id, revision=None, timeout=None, files_metadata=False, ...)</code>.</li>
            <!-- Add more useful HfApi methods and their parameters -->
        </ul>
    </div>
</div>

<br>
<h2>Hugging Face Hub - üß† Inference</h2>
<br>
<div class="container4">
    <div class="section">
        <h2>Inference</h2>
        <ul>
            <li><strong>Inference</strong> is the process of using a <strong>trained model</strong> to make <strong>predictions</strong> on new data. <span class="emoji">üîÆ</span></li>
            <li>Running inference on a dedicated <strong>server</strong> can be more efficient. <span class="emoji">üñ•Ô∏è</span></li>
            <li>The <strong>huggingface_hub</strong> library simplifies calling inference services for hosted models. <span class="emoji">üß∞</span></li>
        </ul>
    </div>
    <div class="section">
        <h2>Inference Services</h2>
        <ul>
            <li><strong>Inference API</strong>: Free, accelerated inference on Hugging Face's infrastructure. <span class="emoji">‚ö°Ô∏è</span> Great for <strong>getting started</strong>, <strong>testing models</strong>, and <strong>prototyping</strong>. <span class="emoji">üî®</span></li>
            <li><strong>Inference Endpoints</strong>: Easily <strong>deploy models</strong> to production with fully managed infrastructure on your <strong>cloud provider</strong>. <span class="emoji">‚òÅÔ∏è</span></li>
        </ul>
    </div>
    <div class="section">
        <h2>Inference Client</h2>
        <p>The <strong>InferenceClient</strong> object connects to inference services. <span class="emoji">üîå</span></p>
        <ul>
            <li><code><strong>model</strong></code>: Model ID (e.g., <code>"meta-llama/Meta-Llama-3-8B-Instruct"</code>) or Inference Endpoint URL. <span class="emoji">üÜî</span></li>
            <li><code><strong>token</strong></code>: Hugging Face <strong>authentication token</strong>. <span class="emoji">üîë</span></li>
            <li><code><strong>timeout</strong></code>: Maximum wait time for server response. <span class="emoji">‚è±Ô∏è</span></li>
        </ul>
    </div>
    <div class="section">
    <h2>Async Inference Client</h2>
    <p>An <strong>asynchronous</strong> version using <strong>asyncio</strong> and <strong>aiohttp</strong>. <span class="emoji">üí´</span></p>
    <ul>
        <li>Install: <code>pip install --upgrade huggingface_hub[inference]</code></li>
        <li>Similar methods to the synchronous client, but with <code><strong>await</strong></code>. <span class="emoji">‚è≥</span></li>
    </ul>
    </div>
</div>
<br>

<div class="container2">
    <div class="section">
        <h2>Key Methods - More Details</h2>
        <ul>
            <li><code><strong>audio_classification(audio, model)</strong></code>: Classifies audio content into predefined categories. üéß
                <ul>
                    <li><code>audio</code>: The audio content to classify (file path, bytes, URL).</li>
                    <li><code>model</code>: The audio classification model to use.</li>
                    <li>Returns: A list of predicted labels with confidence scores.</li>
                </ul>
            </li>
            <li><code><strong>automatic_speech_recognition(audio, model)</strong></code>: Transcribes audio to text (ASR). üé§
                <ul>
                    <li><code>audio</code>: The audio content to transcribe (file path, bytes, URL).</li>
                    <li><code>model</code>: The ASR model to use.</li>
                    <li>Returns: The transcribed text, potentially with timestamps.</li>
                </ul>
            </li>
            <li><code><strong>chat_completion(messages, model, ...)</strong></code>: Generates conversational responses in a chat-like format. üí¨
                <ul>
                    <li><code>messages</code>: A list of chat messages with roles (user, assistant, system).</li>
                    <li><code>model</code>: The conversational model to use.</li>
                    <li>Additional parameters: <code>max_tokens</code>, <code>temperature</code>, etc.</li>
                    <li>Returns: The generated chat response.</li>
                </ul>
            </li>
            <li><code><strong>feature_extraction(text, model)</strong></code>: Generates numerical representations (embeddings) of text. üî¢
                <ul>
                    <li><code>text</code>: The text to embed.</li>
                    <li><code>model</code>: The text embedding model to use.</li>
                    <li>Returns: A numerical vector representing the text.</li>
                </ul>
            </li>
            <li><code><strong>image_classification(image, model)</strong></code>: Classifies images into predefined categories. üñºÔ∏è
                <ul>
                    <li><code>image</code>: The image to classify (file path, bytes, URL).</li>
                    <li><code>model</code>: The image classification model to use.</li>
                    <li>Returns: A list of predicted labels with confidence scores.</li>
                </ul>
            </li>
            <li><code><strong>text_generation(prompt, model, ...)</strong></code>: Generates text based on a given prompt. ‚úçÔ∏è
                <ul>
                    <li><code>prompt</code>: The starting text for the generation.</li>
                    <li><code>model</code>: The text generation model to use.</li>
                    <li>Additional parameters: <code>max_new_tokens</code>, <code>temperature</code>, etc.</li>
                    <li>Returns: The generated text.</li>
                </ul>
            </li>
            <li><code><strong>text_to_image(prompt, model, ...)</strong></code>: Generates images from text descriptions. üé®
                <ul>
                    <li><code>prompt</code>: The text description of the image to generate.</li>
                    <li><code>model</code>: The text-to-image model to use.</li>
                    <li>Additional parameters: <code>height</code>, <code>width</code>, etc.</li>
                    <li>Returns: The generated image.</li>
                </ul>
            </li>
            <li><code><strong>translation(text, model, src_lang, tgt_lang)</strong></code>: Translates text from one language to another. üåê
                <ul>
                    <li><code>text</code>: The text to translate.</li>
                    <li><code>model</code>: The translation model to use.</li>
                    <li><code>src_lang</code>: The source language (optional).</li>
                    <li><code>tgt_lang</code>: The target language (optional).</li>
                    <li>Returns: The translated text.</li>
                </ul>
            </li>
        </ul>
    </div>

    <div class="section">
    <h2>Complete Example: Text-to-Image üñºÔ∏è</h2>
        <pre><code><strong>from huggingface_hub import InferenceClient</strong>
<strong>Initialize</strong> the InferenceClient
client = InferenceClient(token="YOUR_HUGGING_FACE_TOKEN")

<strong>Generate an image</strong>
image = client.text_to_image(
    prompt="A cat wearing a top hat riding a unicycle on a tightrope",
    model="stabilityai/stable-diffusion-2-1",  # Specify the text-to-image model
    height=512,  # Optional: Set image height
    width=512,   # Optional: Set image width
)

<strong>Save the image</strong>
image.save("cat_unicycle.png")
</code></pre>

<h2>Complete Example: Text Generation ‚úçÔ∏è</h2>
<pre><code><strong>from huggingface_hub import InferenceClient</strong>
<strong>Initialize</strong> the InferenceClient
client = InferenceClient(token="YOUR_HUGGING_FACE_TOKEN")

<strong>Generate text</strong>
generated_text = client.text_generation(
    prompt="Once upon a time, in a land far away, ",
    model="gpt2",  # Specify the text generation model
    max_new_tokens=50,  # Limit the length of generated text
    temperature=0.7,  # Control the randomness (higher = more random)
)

<strong>Print the generated text</strong>
print(generated_text)
</code></pre>



</div>
</div>        
    <br>
    <h2>Tokenizers ü§ó</h2>
    
    <div class="container">
        <div class="section">
            <h2>üåü Main Features</h2>
            <ul>
                <li><strong>Fast Training</strong>: Tokenizes 1GB of text in under 20 seconds.</li>
                <li><strong>Versatile</strong>: Supports different tokenization models.</li>
                <li><strong>Full Alignment Tracking</strong>: Maintains correspondence with the original text.</li>
                <li><strong>Pre-Processing Included</strong>: Handles truncation, padding, and special tokens.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üöÄ Quick Tour</h2>
            <ul>
                <li><strong>Build Tokenizer</strong>: Use the library to train a new tokenizer.</li>
                <li><strong>Train with BPE</strong>: Uses merge rules to build tokens.</li>
                <li><strong>Encode Text</strong>: Tokenize and retrieve IDs with <code>Tokenizer.encode</code>.</li>
                <li><strong>Post-Processing</strong>: Automatically adds special tokens.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üîß Installation</h2>
            <ul>
                <li><strong>Using Pip</strong>: <code>pip install tokenizers</code>.</li>
                <li><strong>From Source</strong>: Requires Rust. Use <code>git clone</code> and compile.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üß∞ Pre-Tokenizers</h2>
            <ul>
                <li><strong>Whitespace</strong>: Splits on spaces.</li>
                <li><strong>Punctuation</strong>: Isolates punctuation marks.</li>
                <li><strong>ByteLevel</strong>: Encodes at the byte-level.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>‚öôÔ∏è Models</h2>
            <ul>
                <li><strong>BPE</strong>: Merges frequent pairs of characters.</li>
                <li><strong>WordPiece</strong>: Splits words into tokens.</li>
                <li><strong>Unigram</strong>: Finds the most probable subword units.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üîÑ Post-Processors</h2>
            <ul>
                <li><strong>TemplateProcessing</strong>: Adds special tokens for models.</li>
                <li><strong>Decoders</strong>: Converts IDs back to text.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üõ†Ô∏è Normalizers</h2>
            <ul>
                <li><strong>NFD</strong>: Decomposes Unicode characters.</li>
                <li><strong>Lowercase</strong>: Converts text to lowercase.</li>
                <li><strong>StripAccents</strong>: Removes accent marks.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üß™ Training from Memory</h2>
            <ul>
                <li><strong>Basic Method</strong>: Use any Python iterator.</li>
                <li><strong>Using Datasets</strong>: Integrate with ü§ó Datasets library.</li>
                <li><strong>From Gzip Files</strong>: Train directly from compressed files.</li>
            </ul>
        </div>
        
        <div class="section">
            <h2>üìö Components</h2>
            <ul>
                <li><strong>Normalizers</strong>: Prepare text for tokenization.</li>
                <li><strong>Pre-tokenizers</strong>: Split input text into words.</li>
                <li><strong>Models</strong>: The core algorithm for tokenization.</li>
            </ul>
        </div>
    </div>
    
        <br>
    <h2>ü§ó Transformers Cheatsheet</h2>
    <div class="container">
        <div class="section">
            <h2>Overview</h2>
            <ul>
                <li><strong>Transformers</strong> are state-of-the-art models for <strong>NLP, CV, and Audio</strong> tasks.</li>
                <li>Support for <strong>PyTorch, TensorFlow, and JAX</strong> frameworks.</li>
                <li><strong>Pretrained models</strong> reduce compute costs and training time.</li>
                <li>Provides <strong>APIs</strong> to download and train models easily.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Tasks Supported</h2>
            <ul>
                <li><strong>NLP:</strong> Text classification, named entity recognition, question answering, text generation, etc.</li>
                <li><strong>Computer Vision:</strong> Image classification, object detection, segmentation.</li>
                <li><strong>Audio:</strong> Automatic speech recognition, audio classification.</li>
                <li><strong>Multimodal:</strong> Table question answering, OCR, video classification, visual question answering.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Framework Interoperability</h2>
            <ul>
                <li>Switch between <strong>PyTorch, TensorFlow, and JAX</strong> seamlessly.</li>
                <li>Export models to formats like <strong>ONNX</strong> or <strong>TorchScript</strong> for production deployment.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Getting Started</h2>
            <ul>
                <li>Install: <code>pip install transformers datasets evaluate accelerate</code></li>
                <li>Load a pretrained model: <code>pipeline(task="task-name")</code></li>
                <li>Use <strong>AutoModel</strong> and <strong>AutoTokenizer</strong> for flexibility.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Key Pipelines</h2>
            <ul>
                <li><strong>Text Classification:</strong> <code>pipeline(task="sentiment-analysis")</code></li>
                <li><strong>Text Generation:</strong> <code>pipeline(task="text-generation")</code></li>
                <li><strong>Summarization:</strong> <code>pipeline(task="summarization")</code></li>
                <li><strong>Image Classification:</strong> <code>pipeline(task="image-classification")</code></li>
                <li><strong>Image Segmentation:</strong> <code>pipeline(task="image-segmentation")</code></li>
                <li><strong>Object Detection:</strong> <code>pipeline(task="object-detection")</code></li>
                <li><strong>Audio Classification:</strong> <code>pipeline(task="audio-classification")</code></li>
                <li><strong>Speech Recognition:</strong> <code>pipeline(task="automatic-speech-recognition")</code></li>
                <li><strong>Visual Question Answering:</strong> <code>pipeline(task="vqa")</code></li>
            </ul>
        </div>
        <div class="section">
            <h2>Auto Classes</h2>
            <ul>
                <li><strong>AutoTokenizer:</strong> Preprocess text into numerical arrays.</li>
                <li><strong>AutoModel:</strong> Load a model for specific tasks (e.g., classification, generation).</li>
                <li><strong>AutoConfig:</strong> Customize model architecture (e.g., number of layers, attention heads).</li>
            </ul>
        </div>
        <div class="section">
            <h2>Model Training</h2>
            <ul>
                <li>Use <strong>Trainer</strong> class for PyTorch optimized training loop.</li>
                <li>Use <strong>prepare_tf_dataset()</strong> for TensorFlow training with Keras API.</li>
                <li>Supports distributed training, mixed precision, and callbacks for custom functionality.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Installation Tips</h2>
            <ul>
                <li><strong>Install from pip:</strong> <code>pip install transformers</code></li>
                <li><strong>Install with dependencies:</strong> <code>pip install 'transformers[torch]'</code></li>
                <li><strong>Install from source:</strong> <code>pip install git+https://github.com/huggingface/transformers</code></li>
                <li><strong>Editable install:</strong> For contributing or using the main version.</li>
            </ul>
        </div>
        <div class="section">
            <h2>Saving and Loading Models</h2>
            <ul>
                <li>Save with <strong>save_pretrained()</strong> and reload with <strong>from_pretrained()</strong>.</li>
                <li>Convert models between frameworks using <strong>from_pt</strong> or <strong>from_tf</strong> parameters.</li>
            </ul>
        </div>
    </div>
</body>
</html>
